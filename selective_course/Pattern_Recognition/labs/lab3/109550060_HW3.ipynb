{"cells":[{"cell_type":"markdown","metadata":{"id":"dkWK2awgaHk-"},"source":["## HW3: Decision Tree, AdaBoost and Random Forest\n","In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n","\n","Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"]},{"cell_type":"markdown","metadata":{"id":"qKiVY6tqaHk_"},"source":["## Load data\n","The dataset is the Heart Disease Data Set from UCI Machine Learning Repository. It is a binary classifiation dataset, the label is stored in `target` column. **Please note that there exist categorical features which need to be [one-hot encoding](https://www.datacamp.com/community/tutorials/categorical-data) before fit into your model!**\n","See follow links for more information\n","https://archive.ics.uci.edu/ml/datasets/heart+Disease"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"d4_cf7TJaHlA","executionInfo":{"status":"ok","timestamp":1651984790293,"user_tz":-480,"elapsed":1147,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n","df = pd.read_csv(file_url)\n","# one-hot encoding\n","df = pd.get_dummies(df)\n","\n","train_idx = np.load('train_idx.npy')\n","test_idx = np.load('test_idx.npy')\n","\n","train_df = df.iloc[train_idx]\n","test_df = df.iloc[test_idx]\n","# turn to numpy array\n","x_train = np.delete(train_df.to_numpy(), 12, axis=1)\n","y_train = train_df.to_numpy()[:,12]\n","x_test = np.delete(test_df.to_numpy(), 12, axis=1)\n","y_test = test_df.to_numpy()[:,12]\n","# number of attributes\n","num_attribute = x_train[0].size"]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"b0yuH2E5aHlB","executionInfo":{"status":"ok","timestamp":1651908994337,"user_tz":-480,"elapsed":310,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}},"outputId":"53b1172b-2ae7-4a99-e55d-d144f80a8d84"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n","136   54    1   2       192   283    0        2      195      0      0.0   \n","232   58    0   4       170   225    1        2      146      1      2.8   \n","233   56    1   2       130   221    0        2      163      0      0.0   \n","184   46    1   4       120   249    0        2      144      0      0.8   \n","84    55    0   2       135   250    0        2      161      0      1.4   \n","\n","     slope  ca  target  thal_1  thal_2  thal_fixed  thal_normal  \\\n","136      1   1       0       0       0           0            0   \n","232      2   2       1       0       0           1            0   \n","233      1   0       0       0       0           0            0   \n","184      1   0       0       0       0           0            0   \n","84       2   0       0       0       0           0            1   \n","\n","     thal_reversible  \n","136                1  \n","232                0  \n","233                1  \n","184                1  \n","84                 0  "],"text/html":["\n","  <div id=\"df-d94bdcfc-8b1c-485d-9411-67fde4961c58\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>cp</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>slope</th>\n","      <th>ca</th>\n","      <th>target</th>\n","      <th>thal_1</th>\n","      <th>thal_2</th>\n","      <th>thal_fixed</th>\n","      <th>thal_normal</th>\n","      <th>thal_reversible</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>136</th>\n","      <td>54</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>192</td>\n","      <td>283</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>195</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>232</th>\n","      <td>58</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>170</td>\n","      <td>225</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>146</td>\n","      <td>1</td>\n","      <td>2.8</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>233</th>\n","      <td>56</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>130</td>\n","      <td>221</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>163</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>184</th>\n","      <td>46</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>120</td>\n","      <td>249</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>144</td>\n","      <td>0</td>\n","      <td>0.8</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>55</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>135</td>\n","      <td>250</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>161</td>\n","      <td>0</td>\n","      <td>1.4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d94bdcfc-8b1c-485d-9411-67fde4961c58')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d94bdcfc-8b1c-485d-9411-67fde4961c58 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d94bdcfc-8b1c-485d-9411-67fde4961c58');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":76}],"source":["train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"669jyqhiaHlB"},"source":["## Question 1\n","Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0-vs-fryaHlC","executionInfo":{"status":"ok","timestamp":1651984792180,"user_tz":-480,"elapsed":234,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"outputs":[],"source":["def gini(sequence):\n","  arr = np.bincount(sequence)\n","  pure = arr/sequence.size\n","  init = 1\n","  for x in pure:\n","    init -= np.square(x)\n","  return init\n","\n","def entropy(sequence):\n","  arr = np.bincount(sequence)\n","  pure = arr/sequence.size\n","  init = 0\n","  for x in pure:\n","    if(x!=0):\n","      init += x*np.log2(x)\n","  return -init"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"RUwgwc-GaHlC","executionInfo":{"status":"ok","timestamp":1651933010356,"user_tz":-480,"elapsed":279,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"outputs":[],"source":["# 1 = class 1,\n","# 2 = class 2\n","data = np.array([1,2,1,1,1,1,2,2,1,1,2])"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1S-axywaHlC","executionInfo":{"status":"ok","timestamp":1651933094849,"user_tz":-480,"elapsed":436,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}},"outputId":"0bfb2bb7-aac2-4b7b-b045-c38cde0d33ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gini of data is  0.4628099173553719\n"]}],"source":["print(\"Gini of data is \", gini(data))"]},{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XUfj9MpaHlC","executionInfo":{"status":"ok","timestamp":1651909002432,"user_tz":-480,"elapsed":399,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}},"outputId":"e09271c1-173d-4dcd-cd41-f6064b27cdf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entropy of data is  0.9456603046006401\n"]}],"source":["print(\"Entropy of data is \", entropy(data))"]},{"cell_type":"markdown","metadata":{"id":"UBF1-36QaHlD"},"source":["## Question 2\n","Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n","1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n","2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"]},{"cell_type":"code","source":["# used in question 3\n","feature_used = np.zeros(13)\n","dictionary = dict()\n","for i, x in enumerate(df):\n","  if i<=11:\n","    dictionary[x] = i\n","  else:\n","    dictionary['thal'] = 12"],"metadata":{"id":"12Jwoyt3Pf3K","executionInfo":{"status":"ok","timestamp":1651984794951,"user_tz":-480,"elapsed":241,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Node():\n","  def __init__(self, partition_att=None, partition_val=None, data=None, left=None, right=None):\n","    self.partition_att = partition_att\n","    self.partition_val = partition_val\n","    self.data = data\n","    self.left = left\n","    self.right = right"],"metadata":{"id":"EocJzpiUvk0_","executionInfo":{"status":"ok","timestamp":1651984801186,"user_tz":-480,"elapsed":270,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"fiHf9hdHaHlD","executionInfo":{"status":"ok","timestamp":1651984802296,"user_tz":-480,"elapsed":1,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"outputs":[],"source":["class DecisionTree():\n","  def __init__(self,\n","        criterion='gini', max_depth=None,\n","        x_train=None, y_train=None,\n","        x_test=None, y_test=None):\n","    # numpy arrays\n","    self.x_train = x_train\n","    self.y_train = y_train\n","    self.x_test = x_test\n","    self.y_test = y_test\n","    # index stored in the node\n","    self.train_idx = np.arange(x_train.size/x_train[0].size, dtype=int)\n","    self.test_idx = np.arange(x_test.size/x_test[0].size, dtype=int)\n","    self.node = self.build_tree(criterion, max_depth, self.train_idx)\n","    self.test_res = []\n","    self.wrong_results = []\n","    for test_data in self.test_idx:\n","      self.test_res.append(self.test_tree(criterion, self.node, test_data))\n","    self.test_res = np.array(self.test_res)\n","    for i, res in enumerate(self.test_res):\n","      if self.test_res[i]!=self.y_test[i]:\n","        self.wrong_results.append(i)\n","    self.wrong_results = np.array(self.wrong_results, dtype=int)\n","    self.accuracy = accuracy_score(self.test_res, self.y_test)\n","\n","  def build_tree(self, criterion='gini', depth=None, data=None):\n","    newNode = Node()\n","    if np.all(self.y_train[data]==self.y_train[data][0]):\n","      newNode.data = self.y_train[data][0]\n","      return newNode\n","    if depth==0:\n","      zeros = (np.array(self.y_train[data], dtype=int)==0).sum()\n","      ones = (np.array(self.y_train[data], dtype=int)==1).sum()\n","      newNode.data = 0 if zeros>ones else 1\n","      return newNode\n","      \n","    newNode.data = data\n","    min_criterion = 1\n","\n","    # use i to represent each attribute\n","    for i in range(num_attribute):\n","      # every value of this attribute inside this node \n","      tmp = np.unique(self.x_train[data][:,i])\n","      # use j to partition the attribute\n","      for j in tmp:\n","        # only target value\n","        left = []\n","        right = []\n","        # partition data into two arrays\n","        left_data = []\n","        right_data = []\n","        # use x to represent each value in the current node\n","        for x in data:\n","          if self.x_train[x][i]<j:\n","            left.append(int(self.y_train[x]))\n","            left_data.append(int(x))\n","          else:\n","            right.append(int(self.y_train[x]))\n","            right_data.append(int(x))\n","        left = np.array(left, dtype=int)\n","        right = np.array(right, dtype=int)\n","        left_data = np.array(left_data, dtype=int)\n","        right_data = np.array(right_data, dtype=int)\n","        # use gini as criterion\n","        if criterion=='gini':\n","          cur_gini = gini(left)*left.size/data.size + gini(right)*right.size/data.size\n","          if cur_gini<min_criterion:\n","            min_criterion = cur_gini\n","            newNode.partition_att = i\n","            newNode.partition_val = j\n","            newNode.left_data = left_data\n","            newNode.right_data = right_data\n","        # use entropy as criterion\n","        else:\n","          cur_entropy = entropy(left)*left.size/data.size + entropy(right)*right.size/data.size\n","          if cur_entropy<min_criterion:\n","            min_criterion = cur_entropy\n","            newNode.partition_att = i\n","            newNode.partition_val = j\n","            newNode.left_data = left_data\n","            newNode.right_data = right_data\n","    if newNode.partition_att<=11:\n","      feature_used[newNode.partition_att]+=1\n","    else:\n","      feature_used[12]+=1\n","    newNode.left = self.build_tree(criterion, depth-1 if depth!=None else None, newNode.left_data)\n","    newNode.right = self.build_tree(criterion, depth-1 if depth!=None else None, newNode.right_data)\n","    zeros = (np.array(self.y_train[data], dtype=int)==0).sum()\n","    ones = (np.array(self.y_train[data], dtype=int)==1).sum()\n","    newNode.data = 0 if zeros>ones else 1\n","    return newNode\n","\n","  def test_tree(self, criterion='gini', node=None, data=None):\n","    if node.left==None:\n","      return node.data\n","    if self.x_test[data][node.partition_att]<node.partition_val:\n","      return self.test_tree(criterion, node.left, data)\n","    else:\n","      return self.test_tree(criterion, node.right, data)"]},{"cell_type":"markdown","metadata":{"id":"JhpQNPaCaHlD"},"source":["### Question 2.1\n","Using `criterion=gini`, showing the accuracy score of test data by `max_depth=3` and `max_depth=10`, respectively.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcmRmaEtaHlD","executionInfo":{"status":"ok","timestamp":1651985510190,"user_tz":-480,"elapsed":1266,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}},"outputId":"e8125067-4fa6-49e0-8405-48346683c1ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.78 0.69\n"]}],"source":["clf_depth3 = DecisionTree('gini', 3, \n","              x_train, y_train\n","              ,x_test, y_test)\n","clf_depth10 = DecisionTree('gini', 10, \n","              x_train, y_train\n","              ,x_test, y_test)\n","print(clf_depth3.accuracy, clf_depth10.accuracy)"]},{"cell_type":"markdown","metadata":{"id":"RWPS714KaHlE"},"source":["### Question 2.2\n","Using `max_depth=3`, showing the accuracy score of test data by `criterion=gini` and `criterion=entropy`, respectively.\n"]},{"cell_type":"code","execution_count":191,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QY8HPMkHaHlE","executionInfo":{"status":"ok","timestamp":1651944863591,"user_tz":-480,"elapsed":716,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}},"outputId":"0851992e-4ed8-4e5a-fc79-2d8215e4e377"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.78 0.76\n"]}],"source":["clf_gini = DecisionTree('gini', 3,\n","            x_train, y_train,\n","            x_test, y_test)\n","clf_entropy = DecisionTree('entropy', 3,\n","            x_train, y_train,\n","            x_test, y_test)\n","print(clf_gini.accuracy, clf_entropy.accuracy)"]},{"cell_type":"markdown","metadata":{"id":"0b1jqfUDaHlE"},"source":["- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n","- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n","- Hint: You can use the recursive method to build the nodes\n"]},{"cell_type":"markdown","metadata":{"id":"GjK6sNDkaHlE"},"source":["## Question 3\n","Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n","\n","- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n","\n","![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"]},{"cell_type":"code","source":["plt.barh(range(len(feature_used)), \n","         feature_used,\n","         tick_label=list(dictionary.keys()))\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"LESpnYFFPaCa","executionInfo":{"status":"ok","timestamp":1651933161895,"user_tz":-480,"elapsed":442,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}},"outputId":"89d95ad0-ea2e-4bd0-b2df-0441a0db75be"},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"CdtGixafaHlF"},"source":["## Question 4\n","implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n","1. **n_estimators**: The maximum number of estimators at which boosting is terminated"]},{"cell_type":"code","execution_count":192,"metadata":{"id":"Q4gDg3KKaHlF","executionInfo":{"status":"ok","timestamp":1651944890796,"user_tz":-480,"elapsed":295,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"outputs":[],"source":["class AdaBoost():\n","  def __init__(self, n_estimators):\n","    model1 = DecisionTree('gini', 1,\n","            x_train, y_train,\n","            x_train, y_train)\n","    model2 = DecisionTree('entropy', 2,\n","            x_train, y_train,\n","            x_train, y_train)\n","    x_train_num = int(x_train.size/x_train[0].size)\n","    self.weights = np.full(x_train_num, 1/x_train_num)\n","    self.alphas = []\n","    self.classifiers = []\n","    train_true = np.copy(y_train)\n","    for i, x in enumerate(train_true):\n","      if x==0:\n","        train_true[i] = -1\n","    self.test_res = []\n","    for iter in range(n_estimators):     \n","      fir_err = np.sum(self.weights[model1.wrong_results])\n","      sec_err = np.sum(self.weights[model2.wrong_results])   \n","      min_err = min(fir_err, sec_err)\n","      # result of classifier this round\n","      cur_res = np.array([])\n","      if min_err==fir_err:\n","        self.classifiers.append(1)\n","        cur_res = model1.test_res\n","      elif min_err==sec_err:\n","        self.classifiers.append(2)\n","        cur_res = model2.test_res\n","      self.alphas.append(1/2*np.log((1-min_err)/min_err))\n","      self.weights = self.weights*np.exp(-self.alphas[iter]*train_true*cur_res)\n","      self.weights /= np.sum(self.weights)\n","    self.classifiers = np.array(self.classifiers, dtype=int)\n","    for i, test_data in enumerate(y_test):\n","      # to record the data should be classified to which class\n","      classifier_res = []\n","      for round in range(n_estimators):\n","        cur_classifier = (DecisionTree('gini', 1, x_train, y_train, np.array([x_test[i]]), np.array([y_test[i]])) \n","                  if self.classifiers[round]==1 else\n","                  DecisionTree('entropy', 2, x_train, y_train, np.array([x_test[i]]), np.array([y_test[i]]))) \n","        classifier_res.append(-1 if cur_classifier.test_res[0]==0 else 1)\n","      \n","      self.test_res.append(1) if np.sum(np.dot(self.alphas, classifier_res)) >= 0 else self.test_res.append(0)\n","    self.accuracy = accuracy_score(self.test_res, y_test) "]},{"cell_type":"code","execution_count":193,"metadata":{"id":"xE6lSSpEaHlF","executionInfo":{"status":"ok","timestamp":1651944940077,"user_tz":-480,"elapsed":46890,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"outputs":[],"source":["es_ten = AdaBoost(10)\n","es_hun = AdaBoost(100)"]},{"cell_type":"markdown","metadata":{"id":"WJlb4WBNaHlF"},"source":["### Question 4.1\n","Show the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"]},{"cell_type":"code","execution_count":275,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJ8Zz5OiaHlF","executionInfo":{"status":"ok","timestamp":1651947618113,"user_tz":-480,"elapsed":281,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}},"outputId":"2c5aa00c-3e40-4485-8c3f-065d8585acde"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.78 0.78\n"]}],"source":["print(es_ten.accuracy, es_hun.accuracy)"]},{"cell_type":"markdown","metadata":{"id":"WDGJP872aHlG"},"source":["## Question 5\n","implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n","\n","1. **n_estimators**: The number of trees in the forest. \n","2. **max_features**: The number of random select features to consider when looking for the best split\n","3. **bootstrap**: Whether bootstrap samples are used when building tree\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"wT84mH_1aHlG","executionInfo":{"status":"ok","timestamp":1651986187552,"user_tz":-480,"elapsed":233,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"outputs":[],"source":["class RandomForest():\n","  def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini',\n","          max_depth=None ,x_train=None, y_train=None, x_test=None, y_test=None):\n","    self.n_estimators = n_estimators\n","    self.max_features = max_features\n","    self.boostrap = boostrap\n","    self.trees = []\n","    # numpy arrays\n","    self.x_train = x_train\n","    self.y_train = y_train\n","    self.x_test = x_test\n","    self.y_test = y_test\n","    # index stored in the node\n","    self.train_idx = np.arange(x_train.size/x_train[0].size, dtype=int)\n","    self.test_idx = np.arange(x_test.size/x_test[0].size, dtype=int)\n","    for i in range(n_estimators):\n","      newNode = self.build_tree(criterion, max_depth, self.train_idx)\n","      self.trees.append(newNode)\n","    self.test_res = []\n","    \n","    for test_data in self.test_idx:\n","      zero = 0\n","      one = 0\n","      for tree in self.trees:\n","        if self.test_tree(criterion, tree, test_data)==1:\n","          one += 1\n","        else:\n","          zero += 1\n","      self.test_res.append(1 if one>zero else 0)\n","    self.test_res = np.array(self.test_res)\n","    self.accuracy = accuracy_score(self.test_res, self.y_test)\n","\n","  def build_tree(self, criterion='gini', depth=None, data=None):\n","    newNode = Node()\n","    if data.size==0:\n","      newNode.data = 0\n","      return newNode\n","    if np.all(self.y_train[data]==self.y_train[data][0]):\n","      newNode.data = self.y_train[data][0]\n","      return newNode\n","    if depth==0:\n","      tmp = np.bincount(np.array(self.y_train[data], dtype=int))\n","      newNode.data = 0 if tmp[0]>tmp[1] else 1\n","      return newNode\n","      \n","    newNode.data = data\n","    min_criterion = 1\n","    # listing attribute to use\n","    arr = np.array([])\n","\n","    # use i to represent each attribute\n","    if self.boostrap:\n","      arr = np.random.choice(range(num_attribute), int(self.max_features))\n","    else:\n","      arr = np.random.sample(range(num_attribute), int(self.max_features))\n","    for i in arr:\n","      # every value of this attribute inside this node \n","      tmp = np.unique(self.x_train[data][:,i])\n","      # use j to partition the attribute\n","      for j in tmp:\n","        # only target value\n","        left = []\n","        right = []\n","        # partition data into two arrays\n","        left_data = []\n","        right_data = []\n","        # use x to represent each value in the current node\n","        for x in data:\n","          if self.x_train[x][i]<j:\n","            left.append(int(self.y_train[x]))\n","            left_data.append(int(x))\n","          else:\n","            right.append(int(self.y_train[x]))\n","            right_data.append(int(x))\n","        left = np.array(left, dtype=int)\n","        right = np.array(right, dtype=int)\n","        left_data = np.array(left_data, dtype=int)\n","        right_data = np.array(right_data, dtype=int)\n","        # use gini as criterion\n","        if criterion=='gini':\n","          cur_gini = gini(left)*left.size/data.size + gini(right)*right.size/data.size\n","          if cur_gini<min_criterion:\n","            min_criterion = cur_gini\n","            newNode.partition_att = i\n","            newNode.partition_val = j\n","            newNode.left_data = left_data\n","            newNode.right_data = right_data\n","        # use entropy as criterion\n","        else:\n","          cur_entropy = entropy(left)*left.size/data.size + entropy(right)*right.size/data.size\n","          if cur_entropy<min_criterion:\n","            min_criterion = cur_entropy\n","            newNode.partition_att = i\n","            newNode.partition_val = j\n","            newNode.left_data = left_data\n","            newNode.right_data = right_data\n","    if newNode.partition_att<=11:\n","      feature_used[newNode.partition_att]+=1\n","    else:\n","      feature_used[12]+=1\n","    newNode.left = self.build_tree(criterion, depth-1 if depth!=None else None, newNode.left_data)\n","    newNode.right = self.build_tree(criterion, depth-1 if depth!=None else None, newNode.right_data)\n","    tmp = np.bincount(np.array(self.y_train[data], dtype=int))\n","    newNode.data = 0 if tmp[0]>tmp[1] else 1\n","    return newNode\n","\n","  def test_tree(self, criterion='gini', node=None, data=None):\n","    if node.left==None:\n","      return node.data\n","    if self.x_test[data][node.partition_att]<node.partition_val:\n","      return self.test_tree(criterion, node.left, data)\n","    else:\n","      return self.test_tree(criterion, node.right, data)\n","    "]},{"cell_type":"markdown","metadata":{"id":"p_q8oW6taHlG"},"source":["### Question 5.1\n","Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"5djLJrRwaHlG","executionInfo":{"status":"ok","timestamp":1651986233435,"user_tz":-480,"elapsed":9523,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"outputs":[],"source":["clf_10tree = RandomForest(10, np.sqrt(x_train.shape[1]), True, 'gini',\n","          None ,x_train, y_train, x_test, y_test)\n","clf_100tree = RandomForest(100, np.sqrt(x_train.shape[1]), True, 'gini',\n","          None ,x_train, y_train, x_test, y_test)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dq0Mqtl8aHlG","executionInfo":{"status":"ok","timestamp":1651986234697,"user_tz":-480,"elapsed":296,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}},"outputId":"3a963c2f-ba37-4b0c-891f-737dcbb3ac6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.8 0.74\n"]}],"source":["print(clf_10tree.accuracy, clf_100tree.accuracy)"]},{"cell_type":"markdown","metadata":{"id":"XtlUpJSeaHlH"},"source":["### Question 5.2\n","Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of test data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"somac6pLaHlH","executionInfo":{"status":"ok","timestamp":1651986303193,"user_tz":-480,"elapsed":4254,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}}},"outputs":[],"source":["clf_random_features = RandomForest(10, np.sqrt(x_train.shape[1]), True, 'gini',\n","          None ,x_train, y_train, x_test, y_test)\n","clf_all_features = RandomForest(10, x_train.shape[1], True, 'gini',\n","          None ,x_train, y_train, x_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"9wkdRP0jaHlH"},"source":["- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pu2omK6aHlI","executionInfo":{"status":"ok","timestamp":1651986304271,"user_tz":-480,"elapsed":248,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}},"outputId":"ea2b1593-7171-454c-aa63-4069b2ce1a42"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.82 0.77\n"]}],"source":["print(clf_random_features.accuracy, clf_all_features.accuracy)"]},{"cell_type":"markdown","metadata":{"id":"sD96ve0eaHlI"},"source":["### Question 6.\n","Try you best to get highest test accuracy score by \n","- Feature engineering\n","- Hyperparameter tuning\n","- Implement any other ensemble methods, such as gradient boosting. Please note that you cannot call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CnyJSbpMaHlI"},"outputs":[],"source":["from sklearn.metrics import accuracy_score"]},{"cell_type":"code","execution_count":317,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnAgpngBaHlJ","executionInfo":{"status":"ok","timestamp":1651947983706,"user_tz":-480,"elapsed":1358,"user":{"displayName":"陳星宇","userId":"08613187191843407436"}},"outputId":"d8e15357-79a6-424a-9c70-80ad68b4ada3"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.86\n"]}],"source":["best_result = RandomForest(20, np.sqrt(x_train.shape[1]), True, 'entropy',\n","          3 ,x_train, y_train, x_test, y_test)\n","print(best_result.accuracy)"]},{"cell_type":"markdown","metadata":{"id":"JGeCPNeAaHlJ"},"source":["## Supplementary\n","If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"name":"HW3.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}